{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "import ray\n",
    "from ray.data.preprocessor import Preprocessor\n",
    "from ray.data import Dataset\n",
    "import ray.train as train\n",
    "from ray.train.torch import TorchCheckpoint, TorchTrainer, get_device\n",
    "from ray.train import Checkpoint, session, DataConfig\n",
    "from ray.air.config import CheckpointConfig, DatasetConfig, RunConfig, ScalingConfig\n",
    "\n",
    "# Add the parent directory of the script's current directory to the `sys.path`. This means that when importing modules, Python will also look in the parent directory for any modules that are not found in the current directory or the standard library paths.\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Where to get data\n",
    "DATASET_LOC = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\n",
    "HOLDOUT_LOC = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/holdout.csv\"\n",
    "\n",
    "# Stopwords, effectively reducing dataset size by removing words that occur often but does not add value to learning.\n",
    "nltk.download(\"stopwords\")\n",
    "SW = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String ops to clean things up during pre-processing\n",
    "def clean_text(text, stopwords=SW):\n",
    "    \n",
    "    # Lower case it\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    pattern = re.compile(r'\\b(' + r\"|\".join(stopwords) + r\")\\b\\s*\")\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    # Spacing and filters\n",
    "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)  # add spacing\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)  # remove non alphanumeric chars\n",
    "    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n",
    "    text = text.strip()  # strip white space at the ends\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  #  remove links\n",
    "\n",
    "    return text\n",
    "\n",
    "# Go from words to numericals \n",
    "def tokenize(batch) -> dict:\n",
    "    tok = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "    enc_ip = tok(batch[\"text\"].tolist(), return_tensors=\"np\", padding=\"longest\")\n",
    "    return dict(ids=enc_ip[\"input_ids\"], masks=enc_ip[\"attention_mask\"], targets=np.array(batch[\"tag\"]))\n",
    "\n",
    "# Pre-process data \n",
    "def pp(df, class_to_index):\n",
    "    # Feature 'engineering' with addition of column title & column description.\n",
    "    df[\"text\"] = df.title + \" \" + df.description\n",
    "    # Clean text: lower case, remove stopwords, use regex deal with spaces & special characters\n",
    "    df[\"text\"] = df.text.apply(clean_text)\n",
    "    # Drop unnecessary baggage\n",
    "    df = df.drop(columns=[\"id\", \"created_on\", \"title\", \"description\"], errors=\"ignore\")\n",
    "    # Arrange\n",
    "    df = df[[\"text\", \"tag\"]]\n",
    "    # Map class strings to a number, for machine learning\n",
    "    df[\"tag\"] = df[\"tag\"].map(class_to_index)\n",
    "    # Go from words to numbers, for machine learning\n",
    "    op = tokenize(df)\n",
    "    return op\n",
    "\n",
    "class CustomPP(Preprocessor): \n",
    "    def _fit(self, ds):\n",
    "        tags = ds.unique(column=\"tag\")\n",
    "        self.class_to_index = { tag: i for i, tag in enumerate(tags) }\n",
    "        self.index_to_class = { v: k for k, v in self.class_to_index.items() }\n",
    "    def _transform_pandas(self, batch):\n",
    "        return pp(batch, class_to_index=self.class_to_index)\n",
    "\n",
    "# Split\n",
    "def stratify_split(\n",
    "    ds: Dataset,\n",
    "    stratify: str,\n",
    "    test_size: float,\n",
    "    shuffle: bool = True,\n",
    "    seed: int = 1234,\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"Split a dataset into train and test splits with equal\n",
    "    amounts of data points from each class in the column we\n",
    "    want to stratify on.\n",
    "\n",
    "    Args:\n",
    "        ds (Dataset): Input dataset to split.\n",
    "        stratify (str): Name of column to split on.\n",
    "        test_size (float): Proportion of dataset to split for test set.\n",
    "        shuffle (bool, optional): whether to shuffle the dataset. Defaults to True.\n",
    "        seed (int, optional): seed for shuffling. Defaults to 1234.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dataset, Dataset]: the stratified train and test datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def _add_split(df: pd.DataFrame) -> pd.DataFrame:  # pragma: no cover, used in parent function\n",
    "        \"\"\"Naively split a dataframe into train and test splits.\n",
    "        Add a column specifying whether it's the train or test split.\"\"\"\n",
    "        train, test = train_test_split(df, test_size=test_size, shuffle=shuffle, random_state=seed)\n",
    "        train[\"_split\"] = \"train\"\n",
    "        test[\"_split\"] = \"test\"\n",
    "        return pd.concat([train, test])\n",
    "\n",
    "    def _filter_split(df: pd.DataFrame, split: str) -> pd.DataFrame:  # pragma: no cover, used in parent function\n",
    "        \"\"\"Filter by data points that match the split column's value\n",
    "        and return the dataframe with the _split column dropped.\"\"\"\n",
    "        return df[df[\"_split\"] == split].drop(\"_split\", axis=1)\n",
    "\n",
    "    # Train, test split with stratify\n",
    "    grouped = ds.groupby(stratify).map_groups(_add_split, batch_format=\"pandas\")  # group by each unique value in the column we want to stratify on\n",
    "    train_ds = grouped.map_batches(_filter_split, fn_kwargs={\"split\": \"train\"}, batch_format=\"pandas\")  # combine\n",
    "    test_ds = grouped.map_batches(_filter_split, fn_kwargs={\"split\": \"test\"}, batch_format=\"pandas\")  # combine\n",
    "\n",
    "    # Shuffle each split (required)\n",
    "    train_ds = train_ds.random_shuffle(seed=seed)\n",
    "    test_ds = test_ds.random_shuffle(seed=seed)\n",
    "\n",
    "    return train_ds, test_ds\n",
    "\n",
    "# Set seeds to enable determinism or change otherwise.\n",
    "def set_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    eval(\"setattr(torch.backends.cudnn, 'deterministic', True)\")\n",
    "    eval(\"setattr(torch.backends.cudnn, 'benchmark', False)\")\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Load data and create dataset, ray uses dataset as primitive\n",
    "# Have a way to control how many samples to load, best practice, for testing purposes before pushing on to full data \n",
    "def load_data(num_samples=None):\n",
    "    ds = ray.data.read_csv(DATASET_LOC)\n",
    "    ds = ds.random_shuffle(seed=1234)\n",
    "    ds = ray.data.from_items(ds.take(num_samples)) if num_samples else ds\n",
    "    return ds \n",
    "\n",
    "# Make em all same length \n",
    "def pad_array(arr, dtype=np.int32):\n",
    "    max_len = max(len(row) for row in arr)\n",
    "    padded_arr = np.zeros( (arr.shape[0], max_len), dtype=dtype )\n",
    "    for i, row in enumerate(arr):\n",
    "        padded_arr[i][:len(row)] = row\n",
    "    return padded_arr\n",
    "\n",
    "# Pad and tensor it\n",
    "def collate_fn(batch):\n",
    "    batch[\"ids\"] = pad_array(batch[\"ids\"])\n",
    "    batch[\"masks\"] = pad_array(batch[\"masks\"])\n",
    "    dtypes = { \"ids\" : torch.int32 , \"masks\" : torch.int32 , \"targets\" : torch.int64 }\n",
    "    batch_tensored = {}\n",
    "    for key, array in batch.items():\n",
    "        batch_tensored[key] = torch.as_tensor(array, dtype=dtypes[key], device=get_device())\n",
    "    return batch_tensored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-trained LLM\n",
    "llm = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "embedding_dim = llm.config.hidden_size\n",
    "\n",
    "# Quick test to see if model is working\n",
    "text = \"Transfer learning with transformers for text classification\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "batch = tokenizer([text], return_tensors=\"np\", padding=\"longest\")\n",
    "batch_tensored = { k: torch.tensor(v) for k, v in batch.items() }\n",
    "seq, pool = llm( input_ids=batch_tensored[\"input_ids\"], attention_mask=batch_tensored[\"attention_mask\"] )\n",
    "np.shape(seq), np.shape(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_parameters of FinetunedLLM(\n",
      "  (llm): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=768, out_features=4, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Use LLM as base, then add linear layer for classification\n",
    "\n",
    "class FinetunedLLM(nn.Module):\n",
    "    def __init__(self, llm, dropout_p, embedding_dim, num_classes):\n",
    "        super(FinetunedLLM, self).__init__()\n",
    "        self.llm = llm\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ids, masks = batch[\"ids\"], batch[\"masks\"]\n",
    "        seq, pool = llm( input_ids=ids, attention_mask=masks )\n",
    "        z = self.dropout(pool)\n",
    "        z = self.fc1(z)\n",
    "        return z\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict(self, batch):\n",
    "        self.eval()\n",
    "        z = self(batch)\n",
    "        y_pred = torch.argmax(z, dim=1).cpu().numpy()\n",
    "        return y_pred\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict_proba(self, batch):\n",
    "        self.eval()\n",
    "        z = self(batch)\n",
    "        y_probs = F.softmax(z).cpu().numpy()\n",
    "        return y_probs\n",
    "    \n",
    "model = FinetunedLLM(\n",
    "    llm=llm,\n",
    "    dropout_p = 0.5,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_classes=4,\n",
    ")\n",
    "print(model.named_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 21:08:50,246\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2024-02-02 21:08:52,330\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=24 for stage ReadCSV to satisfy parallelism at least twice the available number of CPUs (12).\n",
      "2024-02-02 21:08:52,330\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 24, each read task output is split into 24 smaller blocks.\n",
      "2024-02-02 21:08:52,331\tINFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV] -> AllToAllOperator[RandomShuffle] -> LimitOperator[limit=1]\n",
      "2024-02-02 21:08:52,332\tINFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=True, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-02-02 21:08:52,333\tINFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f398710c9deb44e081ed974b727b4245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- RandomShuffle 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746a08440b1448c794fa5e94077301f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610060fe97064985a80f080aa8bda9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2597f66f9045088b7019589ba24496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 21:08:54,103\tWARNING util.py:546 -- The argument ``compute`` is deprecated in Ray 2.9. Please specify argument ``concurrency`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
      "2024-02-02 21:08:54,112\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=24 for stage ReadCSV to satisfy parallelism at least twice the available number of CPUs (12).\n",
      "2024-02-02 21:08:54,113\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 24, each read task output is split into 24 smaller blocks.\n",
      "2024-02-02 21:08:54,114\tINFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV] -> AllToAllOperator[RandomShuffle] -> AllToAllOperator[Sort] -> AllToAllOperator[MapBatches(group_fn)->MapBatches(_filter_split)->RandomShuffle] -> TaskPoolMapOperator[MapBatches(fn)] -> LimitOperator[limit=1]\n",
      "2024-02-02 21:08:54,115\tINFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=True, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-02-02 21:08:54,116\tINFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7352ecd902a0440f831fa52fe5402e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- RandomShuffle 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b836aed812445b6bcfa9808be0785ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a061e884e6704c1293a474fd17ffd1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6696409dd7a143dba80a4aba5bc18177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Sort 4:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bd0b91ca374c6895ee073f184f11df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb06136101045c8b907f90e2605965a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 6:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d72518157b45a383b7f02edb0398e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 7:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03dd57a1c954caa9d39ce601a5d4c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(group_fn)->MapBatches(_filter_split)->RandomShuffle 8:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05a405336a84422bea9fd6962939e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 9:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d5dc093016438e8a875b9886ca13e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cab32253dd40f8b7004b5a6ca79b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293c3a26143548c8b8b23b6c75d637f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 0:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 21:08:56,176\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=24 for stage ReadCSV to satisfy parallelism at least twice the available number of CPUs (12).\n",
      "2024-02-02 21:08:56,176\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 24, each read task output is split into 24 smaller blocks.\n",
      "2024-02-02 21:08:56,177\tINFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV] -> AllToAllOperator[RandomShuffle] -> AllToAllOperator[Sort] -> AllToAllOperator[MapBatches(group_fn)->MapBatches(_filter_split)->RandomShuffle] -> TaskPoolMapOperator[MapBatches(fn)] -> AllToAllOperator[Aggregate]\n",
      "2024-02-02 21:08:56,179\tINFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=True, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-02-02 21:08:56,180\tINFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8821eacba72f498bbee9c678039c6c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- RandomShuffle 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c886301f8a94f089e1d69dafd27af30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae463b463a5481598563d7c22d0e0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5a20ac016f48479e1c235b0c5a5d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Sort 4:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea5903a57b4400b91376c611751f937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c88bd944974ab9b76d51e32d43b09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 6:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcbe12876c54d5982525033899fe546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 7:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69283fdcd6f44aefaae95ea8b200656d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(group_fn)->MapBatches(_filter_split)->RandomShuffle 8:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a9bd09b1cc4fd99308c5052743be44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 9:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c73ffc66be4e24b5e188c1ee7bb5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43adda296cb44d66af8c7810c8b4edcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Aggregate 11:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731c07176c7d4712918d4595cab0bc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 12:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41cdb6001e34858bb94788ab412fda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 13:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f282c48ca87b4b7ead63e93a3334e6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c8dca42bcd41aa9cc5481e762af0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 0:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0fa13a7a144e66a12a6e958096d031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 0:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   if isinstance(items[0], TensorArrayElement):\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   return items[0]\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   if isinstance(items[0], TensorArrayElement):\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   return items[0]\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   if isinstance(items[0], TensorArrayElement):\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   return items[0]\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   if isinstance(items[0], TensorArrayElement):\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   return items[0]\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   if isinstance(items[0], TensorArrayElement):\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   return items[0]\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   if isinstance(items[0], TensorArrayElement):\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   return items[0]\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   if isinstance(items[0], TensorArrayElement):\n",
      "\u001b[36m(map pid=2136)\u001b[0m /home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\u001b[36m(map pid=2136)\u001b[0m   return items[0]\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if isinstance(items[0], TensorArrayElement):\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return items[0]\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if isinstance(items[0], TensorArrayElement):\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return items[0]\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if isinstance(items[0], TensorArrayElement):\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return items[0]\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if isinstance(items[0], TensorArrayElement):\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return items[0]\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if isinstance(items[0], TensorArrayElement):\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return items[0]\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if isinstance(items[0], TensorArrayElement):\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return items[0]\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if isinstance(items[0], TensorArrayElement):\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return items[0]\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if isinstance(items[0], TensorArrayElement):\n",
      "/home/tc/miniconda3/envs/mls/lib/python3.10/site-packages/ray/data/_internal/pandas_block.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return items[0]\n"
     ]
    }
   ],
   "source": [
    "# Get dataset ready: Ingest, split, tag mapping, cleaning text, tokenize, pre-processing.\n",
    "\n",
    "# Ensure determinism\n",
    "ray.data.DatasetContext.get_current().execution_options.preserve_order = True\n",
    "\n",
    "# Ingest\n",
    "ds = ray.data.read_csv(\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\")\n",
    "ds = ds.random_shuffle(seed=1234)\n",
    "\n",
    "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=0.2)\n",
    "\n",
    "# Mapping\n",
    "tags = train_ds.unique(column=\"tag\")\n",
    "class_to_index = { tag: i for i, tag in enumerate(tags) }\n",
    "\n",
    "# Distributed preprocessing: mapping of tags to data & applying pre-processing\n",
    "sample_ds = train_ds.map_batches(\n",
    "    pp,\n",
    "    fn_kwargs={\"class_to_index\": class_to_index},\n",
    "    batch_format=\"pandas\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 21:08:57,878\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=24 for stage ReadCSV to satisfy parallelism at least twice the available number of CPUs (12).\n",
      "2024-02-02 21:08:57,879\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 24, each read task output is split into 24 smaller blocks.\n",
      "2024-02-02 21:08:57,880\tINFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV] -> AllToAllOperator[RandomShuffle] -> AllToAllOperator[Sort] -> AllToAllOperator[MapBatches(group_fn)->MapBatches(_filter_split)->RandomShuffle] -> TaskPoolMapOperator[MapBatches(pp)] -> LimitOperator[limit=128]\n",
      "2024-02-02 21:08:57,882\tINFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=True, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-02-02 21:08:57,882\tINFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e9fd9518a64576b6d2b0c4d201af59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- RandomShuffle 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9e54a72243408ca603670c19b143fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d04d2e882943f3828cc93b00e9e6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c94b8c8c5754cb0aa9fa213e34cb89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Sort 4:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ceb2f313c8840eca908ae1647e4ef78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6fd891a1a644258e237280710e73ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 6:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ac56c0d8ae47469532a00bbe321fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 7:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7602e31a774515910a7c639da9cbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(group_fn)->MapBatches(_filter_split)->RandomShuffle 8:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec36d42263134b6980ebbb03af14bb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 9:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e2676e784446129f3ec82a11d303fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b5ca9b634b485da83d2d69c5be1ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea6e01a5bea40a9b68c526a640d777e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 0:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_637/3386752252.py:124: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  batch_tensored[key] = torch.as_tensor(array, dtype=dtypes[key], device=get_device())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([[  102, 15820, 30126,  ...,     0,     0,     0],\n",
       "         [  102, 18715,  4602,  ...,     0,     0,     0],\n",
       "         [  102,  6160,  1923,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  102, 24895, 30111,  ...,     0,     0,     0],\n",
       "         [  102,  2322,  2180,  ...,     0,     0,     0],\n",
       "         [  102,  3267,  4226,  ...,     0,     0,     0]], device='cuda:0',\n",
       "        dtype=torch.int32),\n",
       " 'masks': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32),\n",
       " 'targets': tensor([2, 0, 0, 0, 3, 0, 2, 0, 3, 0, 0, 0, 2, 3, 1, 2, 2, 1, 0, 2, 3, 2, 2, 2,\n",
       "         0, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 3, 0, 0, 0,\n",
       "         2, 1, 2, 2, 0, 3, 3, 0, 0, 2, 2, 2, 1, 0, 2, 3, 2, 1, 2, 0, 2, 2, 0, 2,\n",
       "         0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2, 0, 2, 2,\n",
       "         0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 3, 0, 0, 0, 2, 1,\n",
       "         2, 2, 0, 3, 3, 0, 0, 2], device='cuda:0')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = sample_ds.take_batch(batch_size=128)\n",
    "collate_fn(batch=sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training pipeline\n",
    "\n",
    "def train_step(\n",
    "        ds,\n",
    "        batch_size,\n",
    "        model,\n",
    "        num_classes,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "):\n",
    "    model.train()\n",
    "    loss = 0.0\n",
    "    ds_gen = ds.iterorch_batches(batch_size=batch_size, collate_fn=collate_fn)\n",
    "    for i, batch in enumerate(ds_gen):\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        z = model(batch)\n",
    "        # One-hot vectors, for loss fn\n",
    "        tgts = F.one_hot(batch[\".targets\"], num_classes=num_classes).float()\n",
    "        # Define loss\n",
    "        J = loss_fn(z, tgts)\n",
    "        # Backward pass\n",
    "        J.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # cummulative loss\n",
    "        loss += ( J.detach().item() - loss ) / ( i+1 ) \n",
    "    return loss\n",
    "\n",
    "def eval_step(\n",
    "        ds,\n",
    "        batch_size,\n",
    "        model,\n",
    "        num_classes,\n",
    "        loss_fn,\n",
    "):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    y_trues, y_preds = [], []\n",
    "    ds_gen = ds.iterorch_batches(batch_size=batch_size, collate_fn=collate_fn)\n",
    "    with torch.inference.mode():\n",
    "        for i, batch in enumerate(ds_gen):\n",
    "            z = model(batch)\n",
    "            tgts = F.one_hot(batch[\".targets\"], num_classes=num_classes).float()\n",
    "            J = loss_fn(z, tgts).item()\n",
    "            loss += (J - loss) / (i+1)\n",
    "            y_trues.extend( batch[\"targets\"].cpu().numpy() )\n",
    "            y_preds.extend( torch.argmax(z, dim=1).cpu().numpy() )\n",
    "    return loss, np.vstack(y_trues), np.vstack(y_preds)\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    # HyperP\n",
    "    dropout_p = config[\"dropout_p\"]\n",
    "    lr = config[\"lr\"]\n",
    "    lr_factor = config[\"lr_factor\"]\n",
    "    lr_patience = config[\"lr_patience\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_classes = config[\"num_classes\"]\n",
    "\n",
    "    # Datasets\n",
    "    set_seeds()\n",
    "    train_ds = session.get_dataset_shared(\"train\")\n",
    "    val_ds = session.get_dataset_shared(\"val\")\n",
    "\n",
    "    # Model\n",
    "    llm = BertModel.from_pretained(\"allenai/scibert_scivocab)uncased\", return_dict=False)\n",
    "    model = FinetunedLLM(\n",
    "        llm=llm,\n",
    "        dropout_p=dropout_p,\n",
    "        embedding_dim=llm.config.hidden_size,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    # Trg sub-systems\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\",\n",
    "        factor=lr_factor,\n",
    "        patience=lr_patience,\n",
    "    )\n",
    "\n",
    "    # Trg Loop\n",
    "    batch_size_per_worker = batch_size // session.get_world_size()\n",
    "    for epoch in range(num_epochs):\n",
    "        # Step\n",
    "        train_loss = train_step(\n",
    "            train_ds,\n",
    "            batch_size_per_worker,\n",
    "            model,\n",
    "            num_classes,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "        )\n",
    "\n",
    "        val_loss, _ , _ = eval_step(\n",
    "            val_ds,\n",
    "            batch_size_per_worker,\n",
    "            model,\n",
    "            num_classes,\n",
    "            loss_fn,\n",
    "        )\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Checkpoint it\n",
    "        metrics = dict(\n",
    "            epoch=epoch,\n",
    "            lr=optimizer.param_groups[0][\"lr\"],\n",
    "            train_loss = train_loss,\n",
    "            val_loss = val_loss,\n",
    "        )\n",
    "        checkpoint = TorchCheckpoint.from_model(model=model)\n",
    "        session.report(metrics, checkpoint=checkpoint)\n",
    "    \n",
    "# Trg Config\n",
    "trg_cfg = {\n",
    "    \"dropout_p\": 0.5,\n",
    "    \"lr\": 1e-4,\n",
    "    \"lr_factor\": 0.8,\n",
    "    \"lr_patience\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_classes\": len(tags),\n",
    "}\n",
    "\n",
    "\n",
    "resources_per_worker = {\"CPU\": 0.1, \"GPU\": 0.1}\n",
    "scal_cfg = ScalingConfig(\n",
    "    num_workers=10,\n",
    "    use_gpu=bool(resources_per_worker[\"GPU\"]),\n",
    "    resources_per_worker=resources_per_worker,\n",
    ")\n",
    "\n",
    "checkpoint_cfg = CheckpointConfig(\n",
    "    num_to_keep=1,\n",
    "    checkpoint_score_attribute=\"val_loss\",\n",
    "    checkpoint_score_order=\"min\",\n",
    ")\n",
    "\n",
    "run_cfg = RunConfig(\n",
    "    name=\"llm\",\n",
    "    checkpoint_config=checkpoint_cfg,\n",
    "    local_dir=\"~/ray_results\"\n",
    ")\n",
    "\n",
    "ds_cfg = {\n",
    "    \"train\": DataConfig(),\n",
    "    \"val\": DataConfig(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_data()\n",
    "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=0.2)\n",
    "preprocessor = CustomPP()\n",
    "train_ds = preprocessor.fit_transform(train_ds)\n",
    "val_ds = preprocessor.transform(val_ds)\n",
    "train_ds = train_ds.materialize()\n",
    "val_ds = val_ds.materialize()\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker = train_loop_per_worker,\n",
    "    train_loop_config = trg_cfg,\n",
    "    scaling_config=scal_cfg,\n",
    "    run_config=run_cfg,\n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    # dataset_config=ds_cfg,\n",
    "    # preprocessor=preprocessor,\n",
    ")\n",
    "results = trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
